{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "from collections import deque\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\r\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Bidirectional\r\n",
    "from tensorflow.keras.callbacks import TensorBoard\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ModelCheckpoint\r\n",
    "import time\r\n",
    "from sklearn import preprocessing\r\n",
    "import h5py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# From https://www.tensorflow.org/guide/gpu\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n",
    "\r\n",
    "# gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\r\n",
    "# for device in gpu_devices:\r\n",
    "#     tf.config.experimental.set_memory_growth(device, True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "SEQ_LEN = 180  # how long of a preceeding sequence to collect for RNN\r\n",
    "FUTURE_PERIOD_PREDICT = 5  # how far into the future are we trying to predict?\r\n",
    "RATIO_TO_PREDICT = \"ada_rnn\"\r\n",
    "EPOCHS = 10  # how many passes through our data\r\n",
    "BATCH_SIZE = 128  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\r\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def classify(current, future):\r\n",
    "    if float(future) > float(current):  \r\n",
    "        return 1\r\n",
    "    else: \r\n",
    "        return 0\r\n",
    "\r\n",
    "\r\n",
    "def preprocess_df(df):\r\n",
    "    df = df.drop(\"future\", 1)  \r\n",
    "    \r\n",
    "    for col in df.columns:  \r\n",
    "        if col != \"target\":  \r\n",
    "            df[col] = df[col].pct_change()  \r\n",
    "            df.dropna(inplace=True)  \r\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\r\n",
    "\r\n",
    "    df.dropna(inplace=True)  \r\n",
    "\r\n",
    "    sequential_data = []  \r\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  \r\n",
    "    #print(df.values)\r\n",
    "    for i in df.values:  # iterate over the values\r\n",
    "        prev_days.append([n for n in i[:-1]])  \r\n",
    "        if len(prev_days) == SEQ_LEN:  \r\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  \r\n",
    "    #print(sequential_data)\r\n",
    "    random.shuffle(sequential_data)  \r\n",
    "    #print(sequential_data)\r\n",
    "\r\n",
    "    buys = []  # list that will store our buy sequences and targets\r\n",
    "    sells = []  # list that will store our sell sequences and targets\r\n",
    "\r\n",
    "    for seq, target in sequential_data:  \r\n",
    "        if target == 0:  \r\n",
    "            sells.append([seq, target])  \r\n",
    "        elif target == 1:  \r\n",
    "            buys.append([seq, target])  \r\n",
    "\r\n",
    "    random.shuffle(buys)  \r\n",
    "    random.shuffle(sells)  \r\n",
    "\r\n",
    "    lower = min(len(buys), len(sells)) \r\n",
    "\r\n",
    "    buys = buys[:lower]  \r\n",
    "    sells = sells[:lower]  \r\n",
    "\r\n",
    "    sequential_data = buys+sells  \r\n",
    "    random.shuffle(sequential_data)  \r\n",
    "\r\n",
    "    X = []\r\n",
    "    y = []\r\n",
    "\r\n",
    "    for seq, target in sequential_data:  \r\n",
    "        X.append(seq)  \r\n",
    "        y.append(target)  \r\n",
    "\r\n",
    "    return np.array(X), y  \r\n",
    "\r\n",
    "\r\n",
    "main_df = pd.DataFrame() # begin empty\r\n",
    "\r\n",
    "ratios = [\"ada_rnn\",\"btc_rnn\", \"dot_rnn\", \"eth_rnn\"]  \r\n",
    "for ratio in ratios:  \r\n",
    "\r\n",
    "    ratio = ratio.split('.csv')[0]  \r\n",
    "    print(ratio)\r\n",
    "    dataset = f'autoscraper/data/{ratio}.csv' \r\n",
    "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  \r\n",
    "\r\n",
    "    # rename volume and close to include the ticker so we can still which close/volume is which:\r\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\r\n",
    "\r\n",
    "    df.set_index(\"time\", inplace=True)  \r\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  \r\n",
    "\r\n",
    "    if len(main_df)==0:  \r\n",
    "        main_df = df \r\n",
    "    else:  \r\n",
    "        main_df = main_df.join(df)\r\n",
    "\r\n",
    "main_df.fillna(method=\"ffill\", inplace=True)  \r\n",
    "main_df.dropna(inplace=True)\r\n",
    "main_df.drop_duplicates()\r\n",
    "\r\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\r\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ada_rnn\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "%%time\r\n",
    "# Generate test and train datasets\r\n",
    "times = sorted(main_df.index.values)\r\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.15*len(times))]\r\n",
    "\r\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\r\n",
    "main_df = main_df[(main_df.index < last_5pct)]\r\n",
    "\r\n",
    "x_train, y_train = preprocess_df(main_df)\r\n",
    "x_test, y_test = preprocess_df(validation_main_df)\r\n",
    "\r\n",
    "print(f\"train data: {len(x_train)} validation: {len(x_test)}\")\r\n",
    "print(f\"Dont buys: {y_train.count(0)}, buys: {y_train.count(1)}\")\r\n",
    "print(f\"VALIDATION Dont buys: {y_test.count(0)}, buys: {y_test.count(1)}\")\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train data: 848 validation: 22\n",
      "Dont buys: 424, buys: 424\n",
      "VALIDATION Dont buys: 11, buys: 11\n",
      "Wall time: 313 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Build model\r\n",
    "model = Sequential()\r\n",
    "model.add(Bidirectional(LSTM(128, input_shape=(x_train.shape[1:]), return_sequences=True)))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(BatchNormalization())\r\n",
    "\r\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\r\n",
    "model.add(Dropout(0.1))\r\n",
    "model.add(BatchNormalization())\r\n",
    "\r\n",
    "model.add(Bidirectional(LSTM(128)))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(BatchNormalization())\r\n",
    "\r\n",
    "model.add(Dense(32, activation='relu'))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "\r\n",
    "model.add(Dense(2, activation='softmax'))\r\n",
    "\r\n",
    "\r\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# # '.h5' or empty for tf format, potential avoid missing information\r\n",
    "import os\r\n",
    "\r\n",
    "format_ext = '.h5'  # '.h5' or empty for tf format\r\n",
    "model_path = os.path.join('out', 'mnist-classifier{}'.format(format_ext))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "%%time\r\n",
    "# Compile model\r\n",
    "model.compile(\r\n",
    "    loss='sparse_categorical_crossentropy',\r\n",
    "    optimizer=opt,\r\n",
    "    metrics=['accuracy']\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(NAME)) #Change backslash for windows (logs\\\\) or (logs/) for ubuntu\r\n",
    "filepath = \"RNN_Trained-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\r\n",
    "checkpoint = ModelCheckpoint(\"models\\\\{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Converted labels to arrays before calling model.fit()\r\n",
    "x_train = np.asarray(x_train)\r\n",
    "y_train = np.asarray(y_train)\r\n",
    "x_test = np.asarray(x_test)\r\n",
    "y_test = np.asarray(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "%%time\r\n",
    "# Train model\r\n",
    "history = model.fit(\r\n",
    "    x_train, y_train,\r\n",
    "    batch_size=BATCH_SIZE,\r\n",
    "    epochs=EPOCHS,\r\n",
    "    validation_data=(x_test, y_test),\r\n",
    "    callbacks=[tensorboard, checkpoint]\r\n",
    ")\r\n",
    "# Score model\r\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\r\n",
    "print('Test loss:', score[0])\r\n",
    "print('Test accuracy:', score[1])\r\n",
    "# Save model\r\n",
    "model.save(\"models/{}\".format(NAME))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 848 samples, validate on 22 samples\n",
      "WARNING:tensorflow:From C:\\Users\\findt\\anaconda3\\envs\\goldendwarf\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "848/848 [==============================] - 108s 128ms/sample - loss: 0.8306 - acc: 0.5991 - val_loss: 0.6852 - val_acc: 0.5909\n",
      "Epoch 2/10\n",
      "848/848 [==============================] - 122s 144ms/sample - loss: 0.6516 - acc: 0.6946 - val_loss: 0.6762 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "848/848 [==============================] - 129s 152ms/sample - loss: 0.5230 - acc: 0.7512 - val_loss: 0.6722 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "848/848 [==============================] - 139s 164ms/sample - loss: 0.4801 - acc: 0.7795 - val_loss: 0.6672 - val_acc: 0.6364\n",
      "Epoch 5/10\n",
      "848/848 [==============================] - 140s 165ms/sample - loss: 0.4268 - acc: 0.8137 - val_loss: 0.6633 - val_acc: 0.6364\n",
      "Epoch 6/10\n",
      "848/848 [==============================] - 143s 169ms/sample - loss: 0.3619 - acc: 0.8396 - val_loss: 0.6540 - val_acc: 0.6364\n",
      "Epoch 7/10\n",
      "848/848 [==============================] - 142s 168ms/sample - loss: 0.3342 - acc: 0.8479 - val_loss: 0.6481 - val_acc: 0.6364\n",
      "Epoch 8/10\n",
      "848/848 [==============================] - 172s 203ms/sample - loss: 0.3073 - acc: 0.8679 - val_loss: 0.6368 - val_acc: 0.6364\n",
      "Epoch 9/10\n",
      "848/848 [==============================] - 202s 238ms/sample - loss: 0.2965 - acc: 0.8644 - val_loss: 0.6283 - val_acc: 0.6364\n",
      "Epoch 10/10\n",
      "848/848 [==============================] - 225s 265ms/sample - loss: 0.2555 - acc: 0.8868 - val_loss: 0.6213 - val_acc: 0.6364\n",
      "22/22 [==============================] - 1s 62ms/sample - loss: 0.6213 - acc: 0.6364\n",
      "Test loss: 0.6212663650512695\n",
      "Test accuracy: 0.6363636\n",
      "Wall time: 25min 41s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model.save('model.h5')\r\n",
    "model_json = model.to_json()\r\n",
    "with open(\"model.json\", \"w\") as json_file:\r\n",
    "            json_file.write(model_json)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b32a76a503c9e176ca0049e2d2a9acb4c95fb92710b45fff24f0535732941e1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('goldendwarf': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}