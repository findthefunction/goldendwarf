{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5300b627",
   "metadata": {},
   "source": [
    "# Twitter Scraping & NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf7091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/memme11/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/memme11/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/memme11/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/memme11/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/memme11/opt/anaconda3/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/memme11/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/memme11/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/memme11/opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/memme11/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import json\n",
    "from config import consumer_key, consumer_secret, access_key, access_secret, bearer_token\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Display max column width \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6406b65",
   "metadata": {},
   "source": [
    "## Twitter API (Tweepy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd1b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and gain access to Twitter API\n",
    "def initialize():\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    return api\n",
    "\n",
    "api = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d05fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call on tweepy API and create dataframe\n",
    "def twitter_scrape():\n",
    "    \n",
    "    search_words = (\"bitcoin\", \"etherium\", \"cardano\")\n",
    "    crypto_data = pd.DataFrame()\n",
    "    \n",
    "    def get_data(data):\n",
    "        data = {\n",
    "            'text': data.full_text,\n",
    "            'date': data.created_at,\n",
    "            'followers': data.user.followers_count,\n",
    "            'favourites': data.user.favourites_count,\n",
    "            'retweets': data.retweet_count\n",
    "        }\n",
    "        return data\n",
    "    \n",
    "    for tweets in search_words:\n",
    "        comp_tweets = api.search(q=tweets, lang = 'en', result_type = 'recent', count=250, tweet_mode='extended')\n",
    "        \n",
    "        for tweet in comp_tweets:\n",
    "            row = get_data(tweet)\n",
    "            crypto_data = crypto_data.append(row, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Formatting\n",
    "    # Keep only tweets with over 1000 favourites\n",
    "    crypto_data = crypto_data.loc[crypto_data['favourites']>1000]\n",
    "    \n",
    "    # Clean text column using Regex\n",
    "    crypto_data['cleaned_text'] = crypto_data['text']\n",
    "    clean_text = '(RT) @[\\w]*:|(@[A-Za-z0-9]+)|([^\\,\\!\\.\\'\\%0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)'\n",
    "    crypto_data['cleaned_text'] = crypto_data['cleaned_text'].str.replace(clean_text, \" \", regex=True)\n",
    "    crypto_data['cleaned_text'] = crypto_data['cleaned_text'].str.lower()\n",
    "        \n",
    "    # Convert date dtype to datetime, set index, sort index and drop duplicates\n",
    "    crypto_data['date'] = pd.to_datetime(crypto_data['date'])\n",
    "    crypto_data = crypto_data.set_index('date').sort_index(ascending=False)\n",
    "    crypto_data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    \n",
    "    #Tokenizing\n",
    "    # Tokenizing Functions\n",
    "    def get_wordnet_pos(word):\n",
    "        # Map POS tag to the first character lemmatize() accepts\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        \n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    # Function for tokenizing tweets (already cleaned using regex)\n",
    "    def second_clean(tweet):\n",
    "        tweet = remove_stopwords(tweet) # remove stopwords with Gensim\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokenized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(tweet)]\n",
    "        \n",
    "        # remove left over stop words with nltk\n",
    "        tokenized = [token for token in tokenized if token not in stopwords.words(\"english\")] \n",
    "        \n",
    "        # remove non-alpha characters and keep the words of length >2 only\n",
    "        tokenized = [token for token in tokenized if token.isalpha() and len(token)>2]\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Function for joining tokenized list into string\n",
    "    def combine_tokens(tokenized): \n",
    "        non_tokenized = ' '.join([w for w in tokenized])\n",
    "        return non_tokenized\n",
    "    \n",
    "    # Execute token functions\n",
    "    crypto_data['tokens'] = crypto_data['cleaned_text'].apply(lambda x: second_clean(x))\n",
    "    crypto_data['final_clean'] = crypto_data['tokens'].apply(lambda x: combine_tokens(x))\n",
    "\n",
    "    \n",
    "    # NLP - Vader Sentiment Model \n",
    "    # Sentiment labels function \n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def sentiment_labels(df, feature, value): \n",
    "        df.loc[df[value] > 0,feature] = 'positive'\n",
    "        df.loc[df[value] == 0,feature] = 'neutral'\n",
    "        df.loc[df[value] < 0,feature] = 'negative'\n",
    "    \n",
    "    def vader_sentiment(df):\n",
    "        target_col='cleaned_text'\n",
    "        prefix = 'vader_clean_'\n",
    "        \n",
    "        scores_col=prefix+'scores'\n",
    "        compound_col = prefix+'polarity'\n",
    "        sentiment = prefix+'sentiment'\n",
    "        \n",
    "        df[scores_col] = df[target_col].apply(lambda x:sia.polarity_scores(x))\n",
    "        df[compound_col] = df[scores_col].apply(lambda d: d['compound'])\n",
    "        sentiment_labels(df, sentiment, compound_col)\n",
    "    \n",
    "    # Execute Vader Function\n",
    "    vader_sentiment(crypto_data)\n",
    "    \n",
    "    \n",
    "    # Get sentiment score \n",
    "    vader_values = crypto_data.loc[:, 'vader_clean_polarity']\n",
    "    sentiment_score = round(np.mean(vader_values), 4)\n",
    "    \n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac9858dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0937"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404331c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
